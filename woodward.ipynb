{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00d72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectories_df = pd.read_pickle('../AIS_preprocessed_data/trajectories_2019.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8527b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_df = load_filtered_data(years=('2019',), folder='../AIS_data', min_data_points=100)\n",
    "\n",
    "def sort_row(row):\n",
    "    # load into arrays\n",
    "    e  = np.asarray(row['elapsed_s'])\n",
    "    la = np.asarray(row['LAT'])\n",
    "    lo = np.asarray(row['LON'])\n",
    "    # get the sort‐order\n",
    "    idx = np.argsort(e)\n",
    "    # return a Series so apply(...) will build a DataFrame with matching columns\n",
    "    return pd.Series({\n",
    "        'elapsed_s': e[idx],\n",
    "        'LAT'      : la[idx],\n",
    "        'LON'      : lo[idx]\n",
    "    })\n",
    "\n",
    "# apply and overwrite in one go\n",
    "trajectories_df[['elapsed_s','LAT','LON']] = trajectories_df.apply(sort_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a489ad9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    295090\n",
       "1      1697\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d085fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# threshold in seconds\n",
    "THRESHOLD = 3600\n",
    "\n",
    "def split_row_into_segments(row, threshold=THRESHOLD):\n",
    "    \"\"\"\n",
    "    Given a row with list‐columns ['elapsed_s','LAT','LON'],\n",
    "    split into multiple segments whenever elapsed_s gaps > threshold.\n",
    "    Returns a list of dicts, each dict is one sub-trajectory including break_duration.\n",
    "    \"\"\"\n",
    "    \n",
    "    e  = row['elapsed_s']\n",
    "    la = row['LAT']\n",
    "    lo = row['LON']\n",
    "\n",
    "    # compute gaps and find break points\n",
    "    gaps = np.diff(e)\n",
    "    break_idxs = np.where(gaps > threshold)[0]\n",
    "\n",
    "    # define segment boundaries: starts at 0, each break+1, ends at len(e)\n",
    "    boundaries = np.concatenate([[0], break_idxs + 1, [len(e)]])\n",
    "\n",
    "    segments = []\n",
    "    e_arr = np.asarray(e)\n",
    "\n",
    "    for i in range(len(boundaries) - 1):\n",
    "        start, end = boundaries[i], boundaries[i + 1]\n",
    "        seg = {\n",
    "            'MMSI':        row['MMSI'],\n",
    "            'VesselType':  row['VesselType'],\n",
    "            'Label':       row['Label'],\n",
    "            'elapsed_s':   e_arr[start:end] - e_arr[start],  # normalize to start at 0\n",
    "            'LAT':         la[start:end],\n",
    "            'LON':         lo[start:end],\n",
    "            'weekday':     row['weekday'],\n",
    "            'date':        row['date'],\n",
    "        }\n",
    "\n",
    "        # compute break_duration until next segment, or zero if last\n",
    "        if i < len(boundaries) - 2:\n",
    "            next_start = e_arr[boundaries[i+1]]\n",
    "            current_end = e_arr[boundaries[i+1] - 1]\n",
    "            seg['break_duration'] = int(next_start - current_end)\n",
    "        else:\n",
    "            seg['break_duration'] = 0\n",
    "\n",
    "        # only keep segments with at least two points\n",
    "        if len(seg['elapsed_s']) > 100:\n",
    "            segments.append(seg)\n",
    "\n",
    "    return segments\n",
    "\n",
    "# apply to all rows and flatten\n",
    "all_segments = []\n",
    "for _, row in trajectories_df.iterrows():\n",
    "    all_segments.extend(split_row_into_segments(row))\n",
    "\n",
    "# build new DataFrame\n",
    "segmented_df = pd.DataFrame(all_segments).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c8176ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    300108\n",
       "1      1598\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmented_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b5838ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_counts = segmented_df.groupby('MMSI').size().rename('num_trips')\n",
    "segmented_df = segmented_df.merge(trip_counts, on='MMSI')\n",
    "\n",
    "# define which labels are weekdays vs. weekend\n",
    "weekday_labels = ['Monday','Tuesday','Wednesday','Thursday','Friday']\n",
    "weekend_labels = ['Saturday','Sunday']\n",
    "\n",
    "# weekday trip count per MMSI\n",
    "segmented_df['weekday_trip_count'] = (\n",
    "    segmented_df\n",
    "      .groupby('MMSI')['weekday']\n",
    "      .transform(lambda days: days.isin(weekday_labels).sum())\n",
    ")\n",
    "\n",
    "# weekend trip count per MMSI\n",
    "segmented_df['weekend_trip_count'] = (\n",
    "    segmented_df\n",
    "      .groupby('MMSI')['weekday']\n",
    "      .transform(lambda days: days.isin(weekend_labels).sum())\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a233df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMSI</th>\n",
       "      <th>VesselType</th>\n",
       "      <th>Label</th>\n",
       "      <th>elapsed_s</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>weekday</th>\n",
       "      <th>date</th>\n",
       "      <th>break_duration</th>\n",
       "      <th>num_trips</th>\n",
       "      <th>weekday_trip_count</th>\n",
       "      <th>weekend_trip_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>367659930</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 69, 138, 210, 279, 349, 420, 488, 570, 639...</td>\n",
       "      <td>[30.4289, 30.42774, 30.42675, 30.42573, 30.424...</td>\n",
       "      <td>[-87.99302, -87.99258, -87.9919, -87.99129, -8...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>119</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>367553360</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 70, 149, 220, 301, 379, 449, 559, 630, 709...</td>\n",
       "      <td>[29.01648, 29.01666, 29.01664, 29.01661, 29.01...</td>\n",
       "      <td>[-91.83069, -91.83175, -91.83304, -91.83416, -...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>176</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>367461560</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 65, 131, 197, 263, 335, 402, 467, 533, 600...</td>\n",
       "      <td>[29.38964, 29.38584, 29.38206, 29.37743, 29.37...</td>\n",
       "      <td>[-91.36791, -91.37142, -91.37496, -91.37891, -...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>0</td>\n",
       "      <td>270</td>\n",
       "      <td>191</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>538007067</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 120, 301, 483, 662, 843, 1023, 1204, 1383,...</td>\n",
       "      <td>[28.82299, 28.82325, 28.8236, 28.82388, 28.824...</td>\n",
       "      <td>[-89.33299, -89.33357, -89.3343, -89.33491, -8...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>369053000</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 70, 139, 209, 270, 340, 410, 473, 540, 610...</td>\n",
       "      <td>[30.18058, 30.17847, 30.17641, 30.17425, 30.17...</td>\n",
       "      <td>[-88.56405, -88.56745, -88.57083, -88.57432, -...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301701</th>\n",
       "      <td>538006166</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 81, 224, 301, 401, 510, 581, 651, 750, 831...</td>\n",
       "      <td>[28.91509, 28.91185, 28.90624, 28.90286, 28.89...</td>\n",
       "      <td>[-89.42226, -89.42518, -89.43059, -89.43154, -...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2019-01-13</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301702</th>\n",
       "      <td>538005865</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 69, 130, 337, 440, 620, 681, 811, 891, 104...</td>\n",
       "      <td>[28.91911, 28.91597, 28.91333, 28.90537, 28.90...</td>\n",
       "      <td>[-89.4197, -89.422, -89.4239, -89.43129, -89.4...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2019-01-13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301703</th>\n",
       "      <td>369293000</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 100, 230, 309, 391, 480, 741, 811, 879, 90...</td>\n",
       "      <td>[29.07654, 29.07338, 29.06873, 29.06566, 29.06...</td>\n",
       "      <td>[-90.22863, -90.22913, -90.22987, -90.23059, -...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2019-01-13</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>107</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301704</th>\n",
       "      <td>636017526</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 90, 161, 240, 307, 433, 498, 565, 636, 715...</td>\n",
       "      <td>[30.64235, 30.63707, 30.63251, 30.6274, 30.622...</td>\n",
       "      <td>[-88.03225, -88.03235, -88.03236, -88.03249, -...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2019-01-13</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301705</th>\n",
       "      <td>538004384</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 70, 250, 317, 380, 417, 480, 560, 631, 701...</td>\n",
       "      <td>[28.91971, 28.91616, 28.90798, 28.90495, 28.90...</td>\n",
       "      <td>[-89.41942, -89.42204, -89.42903, -89.43138, -...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2019-01-13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301706 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             MMSI  VesselType  Label  \\\n",
       "0       367659930        31.0      0   \n",
       "1       367553360        30.0      0   \n",
       "2       367461560        90.0      0   \n",
       "3       538007067        70.0      0   \n",
       "4       369053000        90.0      0   \n",
       "...           ...         ...    ...   \n",
       "301701  538006166        80.0      0   \n",
       "301702  538005865        80.0      0   \n",
       "301703  369293000        90.0      0   \n",
       "301704  636017526        70.0      0   \n",
       "301705  538004384        70.0      0   \n",
       "\n",
       "                                                elapsed_s  \\\n",
       "0       [0, 69, 138, 210, 279, 349, 420, 488, 570, 639...   \n",
       "1       [0, 70, 149, 220, 301, 379, 449, 559, 630, 709...   \n",
       "2       [0, 65, 131, 197, 263, 335, 402, 467, 533, 600...   \n",
       "3       [0, 120, 301, 483, 662, 843, 1023, 1204, 1383,...   \n",
       "4       [0, 70, 139, 209, 270, 340, 410, 473, 540, 610...   \n",
       "...                                                   ...   \n",
       "301701  [0, 81, 224, 301, 401, 510, 581, 651, 750, 831...   \n",
       "301702  [0, 69, 130, 337, 440, 620, 681, 811, 891, 104...   \n",
       "301703  [0, 100, 230, 309, 391, 480, 741, 811, 879, 90...   \n",
       "301704  [0, 90, 161, 240, 307, 433, 498, 565, 636, 715...   \n",
       "301705  [0, 70, 250, 317, 380, 417, 480, 560, 631, 701...   \n",
       "\n",
       "                                                      LAT  \\\n",
       "0       [30.4289, 30.42774, 30.42675, 30.42573, 30.424...   \n",
       "1       [29.01648, 29.01666, 29.01664, 29.01661, 29.01...   \n",
       "2       [29.38964, 29.38584, 29.38206, 29.37743, 29.37...   \n",
       "3       [28.82299, 28.82325, 28.8236, 28.82388, 28.824...   \n",
       "4       [30.18058, 30.17847, 30.17641, 30.17425, 30.17...   \n",
       "...                                                   ...   \n",
       "301701  [28.91509, 28.91185, 28.90624, 28.90286, 28.89...   \n",
       "301702  [28.91911, 28.91597, 28.91333, 28.90537, 28.90...   \n",
       "301703  [29.07654, 29.07338, 29.06873, 29.06566, 29.06...   \n",
       "301704  [30.64235, 30.63707, 30.63251, 30.6274, 30.622...   \n",
       "301705  [28.91971, 28.91616, 28.90798, 28.90495, 28.90...   \n",
       "\n",
       "                                                      LON   weekday  \\\n",
       "0       [-87.99302, -87.99258, -87.9919, -87.99129, -8...  Thursday   \n",
       "1       [-91.83069, -91.83175, -91.83304, -91.83416, -...  Thursday   \n",
       "2       [-91.36791, -91.37142, -91.37496, -91.37891, -...  Thursday   \n",
       "3       [-89.33299, -89.33357, -89.3343, -89.33491, -8...  Thursday   \n",
       "4       [-88.56405, -88.56745, -88.57083, -88.57432, -...  Thursday   \n",
       "...                                                   ...       ...   \n",
       "301701  [-89.42226, -89.42518, -89.43059, -89.43154, -...    Sunday   \n",
       "301702  [-89.4197, -89.422, -89.4239, -89.43129, -89.4...    Sunday   \n",
       "301703  [-90.22863, -90.22913, -90.22987, -90.23059, -...    Sunday   \n",
       "301704  [-88.03225, -88.03235, -88.03236, -88.03249, -...    Sunday   \n",
       "301705  [-89.41942, -89.42204, -89.42903, -89.43138, -...    Sunday   \n",
       "\n",
       "             date  break_duration  num_trips  weekday_trip_count  \\\n",
       "0      2019-01-10               0        166                 119   \n",
       "1      2019-01-10               0        250                 176   \n",
       "2      2019-01-10               0        270                 191   \n",
       "3      2019-01-10               0          4                   3   \n",
       "4      2019-01-10               0         41                  29   \n",
       "...           ...             ...        ...                 ...   \n",
       "301701 2019-01-13               0          7                   5   \n",
       "301702 2019-01-13               0          6                   5   \n",
       "301703 2019-01-13               0        148                 107   \n",
       "301704 2019-01-13               0         28                  22   \n",
       "301705 2019-01-13               0          2                   1   \n",
       "\n",
       "        weekend_trip_count  \n",
       "0                       47  \n",
       "1                       74  \n",
       "2                       79  \n",
       "3                        1  \n",
       "4                       12  \n",
       "...                    ...  \n",
       "301701                   2  \n",
       "301702                   1  \n",
       "301703                  41  \n",
       "301704                   6  \n",
       "301705                   1  \n",
       "\n",
       "[301706 rows x 12 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "306e1cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('DailyWeatherData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "103dc5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date      WSPD       GST      WVHT       ATMP\n",
      "0     2019-01-01  5.113460  6.320169  0.991576  18.358261\n",
      "1     2019-01-02  6.913980  8.409737  1.157500  17.867721\n",
      "2     2019-01-03  7.870577  9.824379  1.361417  16.082457\n",
      "3     2019-01-04  7.620777  9.924995  1.101478  14.718592\n",
      "4     2019-01-05  3.521667  4.828611  0.567667  15.243611\n",
      "...          ...       ...       ...       ...        ...\n",
      "1086  2021-12-27  5.953299  7.409201  1.084479  22.644965\n",
      "1087  2021-12-28  6.732009  8.433191  1.286182  22.823268\n",
      "1088  2021-12-29  5.687847  7.206597  1.484479  22.994097\n",
      "1089  2021-12-30  3.135764  4.246354  1.347500  22.643750\n",
      "1090  2021-12-31  4.519444  5.720486  1.105729  22.789757\n",
      "\n",
      "[1091 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load\n",
    "weather_df = pd.read_csv('DailyWeatherData.csv')\n",
    "\n",
    "# 2. Drop any rows where WSPD, GST, WVHT or ATMP is NaN\n",
    "weather_clean = weather_df.dropna(subset=['WSPD','GST','WVHT','ATMP'])\n",
    "\n",
    "# 3. Group by date and compute the mean of those four columns\n",
    "daily_avg = (\n",
    "    weather_clean\n",
    "      .groupby('date')[['WSPD','GST','WVHT','ATMP']]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(daily_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a066b5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        MMSI  VesselType  Label  \\\n",
      "0  367659930        31.0      0   \n",
      "1  367553360        30.0      0   \n",
      "2  367461560        90.0      0   \n",
      "3  538007067        70.0      0   \n",
      "4  369053000        90.0      0   \n",
      "\n",
      "                                           elapsed_s  \\\n",
      "0  [0, 69, 138, 210, 279, 349, 420, 488, 570, 639...   \n",
      "1  [0, 70, 149, 220, 301, 379, 449, 559, 630, 709...   \n",
      "2  [0, 65, 131, 197, 263, 335, 402, 467, 533, 600...   \n",
      "3  [0, 120, 301, 483, 662, 843, 1023, 1204, 1383,...   \n",
      "4  [0, 70, 139, 209, 270, 340, 410, 473, 540, 610...   \n",
      "\n",
      "                                                 LAT  \\\n",
      "0  [30.4289, 30.42774, 30.42675, 30.42573, 30.424...   \n",
      "1  [29.01648, 29.01666, 29.01664, 29.01661, 29.01...   \n",
      "2  [29.38964, 29.38584, 29.38206, 29.37743, 29.37...   \n",
      "3  [28.82299, 28.82325, 28.8236, 28.82388, 28.824...   \n",
      "4  [30.18058, 30.17847, 30.17641, 30.17425, 30.17...   \n",
      "\n",
      "                                                 LON   weekday        date  \\\n",
      "0  [-87.99302, -87.99258, -87.9919, -87.99129, -8...  Thursday  2019-01-10   \n",
      "1  [-91.83069, -91.83175, -91.83304, -91.83416, -...  Thursday  2019-01-10   \n",
      "2  [-91.36791, -91.37142, -91.37496, -91.37891, -...  Thursday  2019-01-10   \n",
      "3  [-89.33299, -89.33357, -89.3343, -89.33491, -8...  Thursday  2019-01-10   \n",
      "4  [-88.56405, -88.56745, -88.57083, -88.57432, -...  Thursday  2019-01-10   \n",
      "\n",
      "   break_duration  num_trips  weekday_trip_count  weekend_trip_count  \\\n",
      "0               0        166                 119                  47   \n",
      "1               0        250                 176                  74   \n",
      "2               0        270                 191                  79   \n",
      "3               0          4                   3                   1   \n",
      "4               0         41                  29                  12   \n",
      "\n",
      "       WSPD       GST    WVHT       ATMP  \n",
      "0  6.904722  8.897778  1.0555  14.238611  \n",
      "1  6.904722  8.897778  1.0555  14.238611  \n",
      "2  6.904722  8.897778  1.0555  14.238611  \n",
      "3  6.904722  8.897778  1.0555  14.238611  \n",
      "4  6.904722  8.897778  1.0555  14.238611  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Ensure your weather daily averages (daily_avg) are loaded:\n",
    "# daily_avg = pd.read_csv(...) or from your previous code\n",
    "\n",
    "# 2. Make sure both DataFrames share a common “date” column of the same type:\n",
    "#    If segmented_df has a timestamp, extract the date part:\n",
    "segmented_df['date'] = pd.to_datetime(segmented_df['date']).dt.date\n",
    "\n",
    "#    And ensure daily_avg.DATE is also a date (not string):\n",
    "daily_avg['date'] = pd.to_datetime(daily_avg['date']).dt.date\n",
    "\n",
    "# 3. Merge on DATE (bringing in WSPD, GST, WVHT, ATMP averages):\n",
    "merged_df = segmented_df.merge(\n",
    "    daily_avg,\n",
    "    on='date',\n",
    "    how='left'    # or 'inner' if you only want segments with matching weather\n",
    ")\n",
    "\n",
    "# 4. (Optional) drop the extra DATE column if you don’t need it:\n",
    "# merged_df = merged_df.drop(columns=['DATE'])\n",
    "\n",
    "print(merged_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ecde1102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    300108\n",
       "1      1598\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f79e8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "COASTAL_POINTS = [(25.9549238885927, -97.146341840721), \n",
    "                  (26.2105963742296, -97.1786084156481), \n",
    "                  (26.4392006473511, -97.2267948818937), \n",
    "                  (26.7131155219549, -97.3144205913896), \n",
    "                  (26.9409506241204, -97.3733392098903), \n",
    "                  (27.2381948299922, -97.3536997038123), \n",
    "                  (27.509565248758, -97.2499956220558), \n",
    "                  (27.8776471339216, -97.0077746348881), \n",
    "                  (28.0539999401663, -96.855022416767), \n",
    "                  (28.3280720859457, -96.4284079754034), \n",
    "                  (28.4969719023136, -96.1916422843358), \n",
    "                  (28.8635868411337, -95.3853300793036), \n",
    "                  (29.2108230117506, -94.9325270042829), \n",
    "                  (29.3231365306908, -94.7208564118929), \n",
    "                  (29.4049143402772, -94.7121277276709), \n",
    "                  (29.4827910105008, -94.5463471921147), \n",
    "                  (29.6592984918529, -94.0739071585848), \n",
    "                  (29.6792073576794, -93.9462501518341), (29.6630909605969, -93.826230743778), (29.7133276704338, -93.7574923555273), (29.7512255330155, -93.4399864669423), (29.7607479179518, -93.1998712835684), (29.6811534311097, -92.9380107569004), (29.5806227705967, -92.6565106907323), (29.5162520975399, -92.2642686354715), (29.5741538015925, -92.1475224839983), (29.5608678187247, -92.0318674180532), (29.4773159297318, -91.8529293914971), (29.4706667956712, -91.7951018585245), (29.5694090083582, -91.6969041610241), (29.6434023797613, -91.8496561349135), (29.7116557246348, -91.7492762663576), (29.7002833853108, -91.640167713579), (29.6196924687798, -91.6205281740788), (29.5181510398553, -91.5059641936618), (29.4754162216433, -91.4132219238004), (29.3746807827744, -91.3619409039944), (29.2243427925429, -91.2517412656884), (29.1044923894996, -90.7483800379835), (29.3044943130288, -90.5574400706218), (29.3301805074153, -90.4406939191486), (29.251198637516, -90.3915950703987), (29.2569102177412, -90.3206745110925), (29.1959702604612, -90.2977617150093), (29.0634922247878, -90.2584819182854), (29.1588178968435, -90.057722181173), (29.2997375962166, -89.8482337598385), (29.2768989322582, -89.6758422464489), (29.1788250621025, -89.4674449106425), (29.2064477008371, -89.3834316847671), (29.1559595208947, -89.3757940860725), (29.1511952028254, -89.4478057309062), (29.0062545714863, -89.4041623097948), (28.9489850882595, -89.4401681322116), (28.9021915070929, -89.3976157966283), (29.0224751597303, -89.2754142175166), (28.948030328351, -89.1401196120713), (29.034877420194, -89.1041137896545), (29.0406010378701, -89.0386486579873), (29.2083524286137, -88.954635072348), (29.4319111022806, -89.2448638227382), (29.4414136102532, -89.380157735065), (29.5316406824672, -89.4587158930652), (29.6872117675113, -89.2404987875087), (29.995091073275, -89.1464772356424), (30.1593768275681, -89.162843518559), (30.1744698581104, -89.2119423673089), (30.0706586329971, -89.3592389135599), (30.0914296066955, -89.4476168413101), (30.2791140829552, -89.3330528608931), (30.3803058562129, -88.9580522807037), (30.3671272452038, -88.8118468199808), (30.3304060353034, -88.7016471816741), (30.3680686334753, -88.6056316552292), (30.3360563490296, -88.585992115729), (30.3351146527177, -88.53143783934), (30.3125112244796, -88.4965231024513), (30.3247553955517, -88.4419688260617), (30.3887768822221, -88.3492265562003), (30.3482975768157, -88.1997478388939), (30.3125112244796, -88.1888369836164), (30.332289509439, -88.1408292203939), (30.2569223017108, -88.1190075098382), (30.2314728019458, -88.3437711285613), (30.2465547826037, -88.0786373453104), (30.2606920373758, -88.1091877400881), (30.337939714482, -88.1277361940602), (30.5436232982486, -88.0718067834058), (30.6450566630599, -88.0379831320445), (30.6506887357231, -87.9583338885163), (30.6121963694031, -87.9234191516275), (30.4157415341932, -87.921236980572), (30.3065333085294, -87.7750315198485), (30.2679047020761, -87.7652117500984), (30.2415154520466, -88.0368920465164), (30.2151191143229, -88.0259811912389), (30.238688023927, -87.6844707666455), (30.287692397843, -87.4542517202834), (30.297692397843, -87.4542517202834)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12ff665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame with explicit column names\n",
    "coast_df = pd.DataFrame(COASTAL_POINTS , columns=['LAT', 'LON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72be6fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "coast_df.to_csv('coastal_points.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53789d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_765637/3095571348.py:94: RuntimeWarning: invalid value encountered in divide\n",
      "  shortest_distances[not_obtuse] = 2 * triangle_area / ab\n",
      "/tmp/ipykernel_765637/3095571348.py:140: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  movement_efficiency = np.sum(distance_diffs[forward_speed>0.1]) / cum_dist[-1] # G3 (the distance traveled when the device is moving more than 0.1 km/min. )\n",
      "/tmp/ipykernel_765637/3095571348.py:175: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  avg_move_speed_kph = (distance_diffs[move_mask].sum() / time_diffs[move_mask].sum()) * 60 if np.any(move_mask) else 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def haversine_np(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radius of Earth in km\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract NumPy arrays for vectorized operations\n",
    "coast_latitudes = coast_df['LAT'].to_numpy()\n",
    "coast_longitudes = coast_df['LON'].to_numpy()\n",
    "num_coast_points = len(coast_df)\n",
    "\n",
    "def compute_distance_to_coast(latitudes: np.ndarray, longitudes: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the shortest distance (km) from each (latitude, longitude) pair \n",
    "    to the coastline polyline defined by COASTAL_POINTS.\n",
    "    Returns an array of same shape as inputs.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are NumPy arrays and of matching shape\n",
    "    # latitudes = np.asarray(latitudes)\n",
    "    # longitudes = np.asarray(longitudes)\n",
    "    # if latitudes.shape != longitudes.shape:\n",
    "    #     raise ValueError(\"Latitude and longitude arrays must have the same shape.\")\n",
    "\n",
    "    # Compute distance from each point to every coastal vertex: shape (..., coast_point_count)\n",
    "    all_distances = haversine_np(\n",
    "        latitudes[..., None],       # extend dims for broadcasting\n",
    "        longitudes[..., None],\n",
    "        coast_latitudes[None, :],\n",
    "        coast_longitudes[None, :]\n",
    "    )\n",
    "\n",
    "    # Find index and distance of the nearest coastal vertex (point A)\n",
    "    nearest_vertex_index = np.argmin(all_distances, axis=-1)\n",
    "    nearest_vertex_distance = np.take_along_axis(\n",
    "        all_distances, \n",
    "        nearest_vertex_index[..., None], \n",
    "        axis=-1\n",
    "    )[..., 0]\n",
    "\n",
    "    # Identify adjacent vertices (A-1 and A+1), clipped to valid range\n",
    "    prev_vertex_index = np.clip(nearest_vertex_index - 1, 0, num_coast_points - 1)\n",
    "    next_vertex_index = np.clip(nearest_vertex_index + 1, 0, num_coast_points - 1)\n",
    "\n",
    "    # Distances from query points to those adjacent vertices\n",
    "    # Flatten indexing for arbitrary shapes\n",
    "    flat_indices = np.arange(nearest_vertex_index.size).reshape(nearest_vertex_index.shape)\n",
    "    prev_vertex_distance = all_distances[flat_indices, prev_vertex_index]\n",
    "    next_vertex_distance = all_distances[flat_indices, next_vertex_index]\n",
    "\n",
    "    # Choose the closer neighbor (point B)\n",
    "    use_prev = prev_vertex_distance < next_vertex_distance\n",
    "    neighbor_index = np.where(use_prev, prev_vertex_index, next_vertex_index)\n",
    "    neighbor_distance = np.where(use_prev, prev_vertex_distance, next_vertex_distance)\n",
    "\n",
    "    # Calculate distance between vertex A and B (segment length)\n",
    "    lat_A = coast_latitudes[nearest_vertex_index]\n",
    "    lon_A = coast_longitudes[nearest_vertex_index]\n",
    "    lat_B = coast_latitudes[neighbor_index]\n",
    "    lon_B = coast_longitudes[neighbor_index]\n",
    "    segment_length = haversine_np(lat_A, lon_A, lat_B, lon_B)\n",
    "\n",
    "    # Determine if the angle at A is obtuse (then closest is A itself)\n",
    "    is_obtuse_at_A = neighbor_distance**2 > segment_length**2 + nearest_vertex_distance**2\n",
    "\n",
    "    # Prepare output distances\n",
    "    shortest_distances = np.empty_like(nearest_vertex_distance)\n",
    "\n",
    "    # Direct distance for obtuse cases\n",
    "    shortest_distances[is_obtuse_at_A] = nearest_vertex_distance[is_obtuse_at_A]\n",
    "\n",
    "    # Perpendicular distance for acute cases via Heron's formula\n",
    "    not_obtuse = ~is_obtuse_at_A\n",
    "    if np.any(not_obtuse):\n",
    "        da = nearest_vertex_distance[not_obtuse]\n",
    "        db = neighbor_distance[not_obtuse]\n",
    "        ab = segment_length[not_obtuse]\n",
    "        s = 0.5 * (ab + da + db)  # semi-perimeter\n",
    "        triangle_area = np.sqrt(s * (s - ab) * (s - da) * (s - db))\n",
    "        shortest_distances[not_obtuse] = 2 * triangle_area / ab\n",
    "\n",
    "    return shortest_distances\n",
    "\n",
    "\n",
    "def extract_features(row):\n",
    "    elapsed = np.array(row['elapsed_s']) / 60.0  # minutes\n",
    "    lat = np.array(row['LAT'])\n",
    "    lon = np.array(row['LON'])\n",
    "\n",
    "    if len(elapsed) < 3:\n",
    "        return pd.DataFrame([{}])  # skip short trips\n",
    "\n",
    "    distances = haversine_np(lat[:-1], lon[:-1], lat[1:], lon[1:])\n",
    "    distance_diffs = np.concatenate([[0], distances])\n",
    "    dt = np.diff(elapsed) + 1e-10\n",
    "    time_diffs = np.concatenate([[0], dt])\n",
    "    cum_time = elapsed\n",
    "    cum_dist = np.cumsum(distance_diffs)\n",
    "\n",
    "\n",
    "\n",
    "    # Forward and backward speed calculations\n",
    "    padded_time = np.concatenate([[cum_time[0] - 5], cum_time, [cum_time[-1] + 5]])\n",
    "    padded_dist = np.concatenate([[cum_dist[0]], cum_dist, [cum_dist[-1]]])\n",
    "    f_dist = interp1d(padded_time, padded_dist, fill_value='extrapolate')\n",
    "\n",
    "    Time_step = 5\n",
    "    forward_speed = (f_dist(cum_time + Time_step) - f_dist(cum_time)) / Time_step\n",
    "    backward_speed = (f_dist(cum_time) - f_dist(cum_time - Time_step)) / Time_step\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    pings_per_minute_in_movement = np.sum(forward_speed > 0.1) # G5 (the number of pings during the trip when the device is moving more than 0.1 km/min.  )\n",
    "    min_speed = np.minimum(forward_speed, backward_speed)\n",
    "    max_speed = np.maximum(forward_speed, backward_speed)\n",
    "\n",
    "    stop_mask = min_speed < 0.025\n",
    "    slow_mask = (min_speed >= 0.025) & (max_speed < 0.1)\n",
    "    move_mask = max_speed >= 0.1\n",
    "\n",
    "    total_time = cum_time[-1] - cum_time[0]\n",
    "    break_duration = row['break_duration']        # G1 placeholder\n",
    "    trip_duration = total_time  # G2 (Trip duration)\n",
    "    movement_efficiency = np.sum(distance_diffs[forward_speed>0.1]) / cum_dist[-1] # G3 (the distance traveled when the device is moving more than 0.1 km/min. )\n",
    "\n",
    "    trips_per_year = row['num_trips']  # G4 placeholder\n",
    "    pings_in_movement = move_mask.sum()\n",
    "    time_in_movement = time_diffs[move_mask].sum()\n",
    "    pings_per_minute_in_movement = pings_in_movement / time_in_movement if time_in_movement > 0 else 0  # G5\n",
    "\n",
    "    def get_durations(mask):\n",
    "        durations, dur = [], 0\n",
    "        for i, flag in enumerate(mask):\n",
    "            dur = dur + time_diffs[i] if flag else 0\n",
    "            if flag and (i == len(mask) - 1 or not mask[i + 1]):\n",
    "                durations.append(dur)\n",
    "        return durations\n",
    "\n",
    "    stop_durations = get_durations(stop_mask)\n",
    "    time_stopped = sum(stop_durations) # Total time stopped (min)\n",
    "    pct_time_stopped = time_stopped / total_time if total_time > 0 else 0 # Percentage of time stopped\n",
    "    number_of_stops = len(stop_durations) # Number of stops\n",
    "    longest_stop = max(stop_durations) if stop_durations else 0 # Longest stop \n",
    "    shortest_stop = min(stop_durations) if stop_durations else 0 # Shortest stop\n",
    "\n",
    "    slow_durations = get_durations(slow_mask)\n",
    "    time_slow = sum(slow_durations) # Total time in slow movement (min) \n",
    "    pct_time_slow = time_slow / total_time if total_time > 0 else 0 # Percentage of time in slow movement (%) \n",
    "    number_of_slow = len(slow_durations) # Number of slow movements\n",
    "    longest_slow = max(slow_durations) if slow_durations else 0 # Longest slow movement\n",
    "    shortest_slow = min(slow_durations) if slow_durations else 0 # Shortest slow movement\n",
    "\n",
    "    move_durations = get_durations(move_mask)\n",
    "    time_move = sum(move_durations) # Total time in movement (min)\n",
    "    pct_time_move = time_move / total_time if total_time > 0 else 0     # Percentage of time in movement (%)\n",
    "    number_of_moves = len(move_durations)  # Number of movements\n",
    "    longest_move = max(move_durations) if move_durations else 0 # Longest movement\n",
    "    shortest_move = min(move_durations) if move_durations else 0 # Shortest movement\n",
    "    avg_move_speed_kph = (distance_diffs[move_mask].sum() / time_diffs[move_mask].sum()) * 60 if np.any(move_mask) else 0\n",
    "    max_move_speed_kph = max_speed[move_mask].max() * 60 if np.any(move_mask) else 0\n",
    "\n",
    "    dist_move = distance_diffs[move_mask].sum()\n",
    "    dist_slow = distance_diffs[slow_mask].sum()\n",
    "\n",
    "    origin_lat, origin_lon = lat[0], lon[0]\n",
    "    from_origin = haversine_np(origin_lat, origin_lon, lat, lon)\n",
    "    dist_from_origin_in_stops = np.average(from_origin[stop_mask], weights=time_diffs[stop_mask]) if np.any(stop_mask) else 0\n",
    "    total_distance = distance_diffs.sum()\n",
    "    dist_from_origin = from_origin[-1]\n",
    "    max_dist_from_origin = from_origin.max()\n",
    "\n",
    "\n",
    "    distances = compute_distance_to_coast(lat, lon)  # This will compute the distance to coast for each point\n",
    "    first_dist_coast = distances[0] # The first point distance from coast (km) \n",
    "    last_dist_coast = distances[-1] # The last point distance from coast (km) \n",
    "    max_dist_coast = np.max(distances) # Max distance traveled from coast (km)\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        'break_duration': break_duration,  # G1\n",
    "        'trip_duration_min': trip_duration,  # G2\n",
    "        'movement_efficiency': movement_efficiency,  # G3\n",
    "        'trips_per_year': trips_per_year,  # G4\n",
    "        'pings_per_minute_in_movement': pings_per_minute_in_movement,  # G5\n",
    "        'time_stopped': time_stopped,\n",
    "        'pct_time_stopped': pct_time_stopped,\n",
    "        'number_of_stops': number_of_stops,\n",
    "        'longest_stop': longest_stop,\n",
    "        'shortest_stop': shortest_stop,\n",
    "        'time_slow': time_slow,\n",
    "        'pct_time_slow': pct_time_slow,\n",
    "        'number_of_slow': number_of_slow,\n",
    "        'longest_slow': longest_slow,\n",
    "        'shortest_slow': shortest_slow,\n",
    "        'time_move': time_move,\n",
    "        'pct_time_move': pct_time_move,\n",
    "        'number_of_moves': number_of_moves,\n",
    "        'longest_move': longest_move,\n",
    "        'shortest_move': shortest_move,\n",
    "        'avg_move_speed_kph': avg_move_speed_kph,\n",
    "        'max_move_speed_kph': max_move_speed_kph,\n",
    "        'distance_move_km': dist_move,\n",
    "        'distance_slow_km': dist_slow,\n",
    "        'dist_from_origin_in_stops': dist_from_origin_in_stops,\n",
    "        'total_distance_km': total_distance,\n",
    "        'dist_from_origin_km': dist_from_origin,\n",
    "        'max_dist_from_origin_km': max_dist_from_origin,\n",
    "        'first_dist_coast_km': first_dist_coast,\n",
    "        'last_dist_coast_km': last_dist_coast,\n",
    "        'max_dist_coast_km': max_dist_coast,\n",
    "        'weekday_trip_count': row['weekday_trip_count'],\n",
    "        'weekend_trip_count': row['weekend_trip_count'],\n",
    "        'gust_speed': row['GST'],\n",
    "        'wave_height': row['WVHT'],\n",
    "        'wind_speed': row['WSPD'],\n",
    "        'air_temp': row['ATMP'],\n",
    "        'Label': row['Label'],\n",
    "    }])\n",
    "data = pd.concat(merged_df.apply(extract_features, axis=1).tolist(), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "14153af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['break_duration', 'trip_duration_min', 'movement_efficiency',\n",
       "       'trips_per_year', 'pings_per_minute_in_movement', 'time_stopped',\n",
       "       'pct_time_stopped', 'number_of_stops', 'longest_stop', 'shortest_stop',\n",
       "       'time_slow', 'pct_time_slow', 'number_of_slow', 'longest_slow',\n",
       "       'shortest_slow', 'time_move', 'pct_time_move', 'number_of_moves',\n",
       "       'longest_move', 'shortest_move', 'avg_move_speed_kph',\n",
       "       'max_move_speed_kph', 'distance_move_km', 'distance_slow_km',\n",
       "       'dist_from_origin_in_stops', 'total_distance_km', 'dist_from_origin_km',\n",
       "       'max_dist_from_origin_km', 'first_dist_coast_km', 'last_dist_coast_km',\n",
       "       'max_dist_coast_km', 'weekday_trip_count', 'gust_speed', 'wave_height',\n",
       "       'wind_speed', 'air_temp', 'num_trips', 'Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6d0909a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data.dropna()\n",
    "\n",
    "y = data_clean['Label']\n",
    "X = data_clean.drop(columns=['Label',\n",
    "                             'trips_per_year',\n",
    "                             'num_trips',\n",
    "                             'break_duration',\n",
    "                            #  'weekday_trip_count',\n",
    "                            #  'weekend_trip_count'\n",
    "                             ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b3dabb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1597)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cf92c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    f1_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y,\n",
    "    stratify=y,\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2) Split train+val into train (60%) and val (20%)\n",
    "# Since X_temp is 80%, using test_size=0.25 on it gives 0.25 * 0.8 = 0.20 overall\n",
    "X_train_orig, X_val, y_train_orig, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    stratify=y_temp,\n",
    "    test_size=0.25,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define the base classifier.\n",
    "# class_weight='balanced' automatically adjusts weights inversely proportional to class frequencies.\n",
    "base_clf = RandomForestClassifier(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9ee5968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to Train, Tune Threshold, and Evaluate ---\n",
    "def train_tune_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, classifier, sampling_strategy_name):\n",
    "    \"\"\"\n",
    "    Trains a classifier, tunes its probability threshold on a validation set,\n",
    "    and reports its performance on a test set.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training labels.\n",
    "        X_val (pd.DataFrame): Validation features (for threshold tuning).\n",
    "        y_val (pd.Series): Validation labels (for threshold tuning).\n",
    "        X_test (pd.DataFrame): Test features (for final evaluation).\n",
    "        y_test (pd.Series): Test labels (for final evaluation).\n",
    "        classifier (sklearn.base.Estimator): The machine learning classifier to use.\n",
    "        sampling_strategy_name (str): A descriptive name for the current sampling strategy\n",
    "                                      (e.g., \"Original Distribution\", \"Undersampled\", \"Oversampled\").\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\\n--- Results for {sampling_strategy_name} Training Data ---\\n{'='*50}\")\n",
    "\n",
    "    clf = classifier.__class__(**classifier.get_params()) # Create a fresh instance to avoid retraining issues\n",
    "    \n",
    "    # Train the classifier on the provided training data\n",
    "    print(f\"Training classifier on {len(X_train)} samples...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # Get probability scores for the validation set\n",
    "    val_scores = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Compute Precision-Recall curve to find the best threshold\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, val_scores)\n",
    "\n",
    "    # Calculate F1-score for each threshold to identify the optimal one\n",
    "    # Add a small epsilon (1e-6) to prevent division by zero for F1 calculation\n",
    "    f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-6)\n",
    "\n",
    "    # Find the threshold that yields the highest F1-score on the validation set\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_f1_val = f1_scores[best_idx]\n",
    "\n",
    "    print(f\"\\nBest validation threshold = {best_threshold:.4f} (F1-score = {best_f1_val:.4f})\")\n",
    "\n",
    "    # Evaluate the classifier on the unseen test set using the best threshold found\n",
    "    test_scores = clf.predict_proba(X_test)[:, 1]\n",
    "    y_test_pred = (test_scores >= best_threshold).astype(int)\n",
    "\n",
    "    print(f\"\\n--- Classification Report on Test Set @ Threshold {best_threshold:.4f} ---\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5740f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- Results for Original Distribution Training Data ---\n",
      "==================================================\n",
      "Training classifier on 178132 samples...\n",
      "Training complete.\n",
      "\n",
      "Best validation threshold = 0.2100 (F1-score = 0.7303)\n",
      "\n",
      "--- Classification Report on Test Set @ Threshold 0.2100 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     59059\n",
      "           1       0.79      0.73      0.76       319\n",
      "\n",
      "    accuracy                           1.00     59378\n",
      "   macro avg       0.90      0.86      0.88     59378\n",
      "weighted avg       1.00      1.00      1.00     59378\n",
      "\n",
      "Saved Original Distribution model to saved_models/model_original_distribution.joblib\n",
      "\n",
      "==================================================\n",
      "--- Results for Undersampled Training Data ---\n",
      "==================================================\n",
      "Training classifier on 1916 samples...\n",
      "Training complete.\n",
      "\n",
      "Best validation threshold = 0.8500 (F1-score = 0.5522)\n",
      "\n",
      "--- Classification Report on Test Set @ Threshold 0.8500 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     59059\n",
      "           1       0.48      0.70      0.57       319\n",
      "\n",
      "    accuracy                           0.99     59378\n",
      "   macro avg       0.74      0.85      0.78     59378\n",
      "weighted avg       1.00      0.99      0.99     59378\n",
      "\n",
      "Saved Undersampled model to saved_models/model_undersampled.joblib\n",
      "\n",
      "==================================================\n",
      "--- Results for Oversampled Training Data ---\n",
      "==================================================\n",
      "Training classifier on 354348 samples...\n",
      "Training complete.\n",
      "\n",
      "Best validation threshold = 0.2700 (F1-score = 0.7395)\n",
      "\n",
      "--- Classification Report on Test Set @ Threshold 0.2700 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     59059\n",
      "           1       0.77      0.74      0.75       319\n",
      "\n",
      "    accuracy                           1.00     59378\n",
      "   macro avg       0.88      0.87      0.88     59378\n",
      "weighted avg       1.00      1.00      1.00     59378\n",
      "\n",
      "Saved Oversampled model to saved_models/model_oversampled.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from joblib import dump\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "# make a directory for your models\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "# Scenario 1: Original Distribution\n",
    "model_orig = train_tune_evaluate(\n",
    "    X_train_orig, y_train_orig,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test,\n",
    "    base_clf,\n",
    "    \"Original Distribution\"\n",
    ")\n",
    "dump(model_orig, \"saved_models/model_original_distribution.joblib\")\n",
    "print(\"Saved Original Distribution model to saved_models/model_original_distribution.joblib\")\n",
    "\n",
    "# Scenario 2: Undersampled Training Data\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train_orig, y_train_orig)\n",
    "\n",
    "model_under = train_tune_evaluate(\n",
    "    X_train_under, y_train_under,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test,\n",
    "    base_clf,\n",
    "    \"Undersampled\"\n",
    ")\n",
    "dump(model_under, \"saved_models/model_undersampled.joblib\")\n",
    "print(\"Saved Undersampled model to saved_models/model_undersampled.joblib\")\n",
    "\n",
    "# Scenario 3: Oversampled Training Data\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train_over, y_train_over = oversampler.fit_resample(X_train_orig, y_train_orig)\n",
    "\n",
    "model_over = train_tune_evaluate(\n",
    "    X_train_over, y_train_over,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test,\n",
    "    base_clf,\n",
    "    \"Oversampled\"\n",
    ")\n",
    "dump(model_over, \"saved_models/model_oversampled.joblib\")\n",
    "print(\"Saved Oversampled model to saved_models/model_oversampled.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4e04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180a02f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recfish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
